{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download and preprocessing\n",
    "\n",
    "This notebook handles data downloading and preprocessing, preparing it for use in tiles.ipynb, locations.ipynb, and/or precalculations.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import dotenv  \n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "scripts_dir = Path(\".\").joinpath(\"src\")\n",
    "import sys\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.insert(0, scripts_dir.resolve().as_posix())\n",
    "\n",
    "from helpers.utils import downloadFile, rm_tree, make_archive, writeReadGCP\n",
    "from helpers.settings import get_settings\n",
    "from helpers.file_handler import FileConventionHandler\n",
    "from pipelines.utils import watch\n",
    "from pipelines.processors import (\n",
    "    set_wdpa_id,\n",
    "    protection_level,\n",
    "    status,\n",
    "    create_year,\n",
    "    calculate_area,\n",
    "    get_mpas,\n",
    "    set_location_iso,\n",
    "    set_fps_classes,\n",
    "    filter_by_methodology,\n",
    "    filter_by_terrestrial,\n",
    "    transform_points,\n",
    "    clean_geometries,\n",
    "    simplify_async,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysettings = get_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eez_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe params\n",
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"eez\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sources\n",
    "## EEZ\n",
    "EEZ_url = \"https://www.marineregions.org/download_file.php\"\n",
    "EEZ_file_name = \"eez_v11.shp\"\n",
    "EEZ_params = {\"name\": \"World_EEZ_v11_20191118.zip\"}\n",
    "EEZ_headers = {\n",
    "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "    \"cookie\": \"PHPSESSID=29190501b4503e4b33725cd6bd01e2c6; vliz_webc=vliz_webc2; jwplayer.captionLabel=Off\",\n",
    "    \"dnt\": \"1\",\n",
    "    \"origin\": \"https://www.marineregions.org\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "}\n",
    "\n",
    "EEZ_body = {\n",
    "    \"name\": \"Jason\",\n",
    "    \"organisation\": \"skytruth\",\n",
    "    \"email\": \"hello@skytruth.com\",\n",
    "    \"country\": \"Spain\",\n",
    "    \"user_category\": \"academia\",\n",
    "    \"purpose_category\": \"Conservation\",\n",
    "    \"agree\": \"1\",\n",
    "}\n",
    "\n",
    "## High seas\n",
    "hs_url = \"https://www.marineregions.org/download_file.php\"\n",
    "hs_file_name = \"High_seas_v1.shp\"\n",
    "hs_params = {\"name\": \"World_High_Seas_v1_20200826.zip\"}\n",
    "hs_headers = {\n",
    "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "    \"cookie\": \"PHPSESSID=29190501b4503e4b33725cd6bd01e2c6; vliz_webc=vliz_webc2; jwplayer.captionLabel=Off\",\n",
    "    \"dnt\": \"1\",\n",
    "    \"origin\": \"https://www.marineregions.org\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "}\n",
    "hs_body = {\n",
    "    \"name\": \"Jason\",\n",
    "    \"organisation\": \"skytruth\",\n",
    "    \"email\": \"hello@skytruth.com\",\n",
    "    \"country\": \"Spain\",\n",
    "    \"user_category\": \"academia\",\n",
    "    \"purpose_category\": \"Conservation\",\n",
    "    \"agree\": \"1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "## download files EEZ & High seas\n",
    "downloadFile(\n",
    "    EEZ_url,\n",
    "    input_path,\n",
    "    EEZ_body,\n",
    "    EEZ_params,\n",
    "    EEZ_headers,\n",
    "    overwrite=force_clean,\n",
    ")\n",
    "downloadFile(hs_url, input_path, hs_body, hs_params, hs_headers, overwrite=force_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzip file if needed & load data\n",
    "unziped_folders = []\n",
    "for idx, path in enumerate(input_path.glob(\"*.zip\")):\n",
    "    unziped_folder = temp_working_path.joinpath(path.stem)\n",
    "    print(unziped_folder)\n",
    "\n",
    "    if unziped_folder.exists() and force_clean:\n",
    "        rm_tree(unziped_folder)\n",
    "\n",
    "    shutil.unpack_archive(path, unziped_folder)\n",
    "\n",
    "    files = [gpd.read_file(file) for file in unziped_folder.rglob(\"*.shp\") if \"boundaries\" not in file.stem]\n",
    "    unziped_folders.append(\n",
    "        pd.concat(files)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, gdf in enumerate(unziped_folders):\n",
    "    print(f\"GeoDataFrame {idx} has {len(gdf)} rows and {len(gdf.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "## set the same structure for both datasets updating the high seas one\n",
    "unziped_folders[0] = (\n",
    "    unziped_folders[0]\n",
    "    .rename(\n",
    "        columns={\"name\": \"GEONAME\", \"area_km2\": \"AREA_KM2\", \"mrgid\": \"MRGID\"},\n",
    "    )\n",
    "    .assign(\n",
    "        POL_TYPE=\"High Seas\",\n",
    "        ISO_SOV1=\"ABNJ\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# merge datasets\n",
    "df = pd.concat(unziped_folders, ignore_index=True)\n",
    "\n",
    "df.drop(\n",
    "    columns=list(\n",
    "        set(df.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"MRGID\",\n",
    "                \"GEONAME\",\n",
    "                \"POL_TYPE\",\n",
    "                \"ISO_SOV1\",\n",
    "                \"ISO_SOV2\",\n",
    "                \"ISO_SOV3\",\n",
    "                \"AREA_KM2\",\n",
    "                \"geometry\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "gpd.GeoDataFrame(\n",
    "    df,\n",
    "    crs=unziped_folders[0].crs,\n",
    ").to_file(filename=output_file.as_posix(), driver=\"ESRI Shapefile\")\n",
    "\n",
    "# zip data\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries gadm intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe params\n",
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"gadm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm_url = \"https://geodata.ucdavis.edu/gadm/gadm4.1/gadm_410-levels.zip\"\n",
    "gadm_file_name = \"gadm_410-levels.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "input_file = downloadFile(\n",
    "    gadm_url,\n",
    "    input_path,\n",
    "    overwrite=force_clean,\n",
    "    file=gadm_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /home/sofia/dev/skytruth-30x30/data/data/gadm/raw/temp_preprocess/gadm_410-levels\n",
      "Removed existing folder: /home/sofia/dev/skytruth-30x30/data/data/gadm/raw/temp_preprocess/gadm_410-levels\n",
      "Unpacked /home/sofia/dev/skytruth-30x30/data/data/gadm/raw/gadm_410-levels.zip to /home/sofia/dev/skytruth-30x30/data/data/gadm/raw/temp_preprocess/gadm_410-levels\n"
     ]
    }
   ],
   "source": [
    "# Check if there is a zip file in the input_path\n",
    "zip_file = next(input_path.glob(\"*.zip\"), None)\n",
    "if zip_file:\n",
    "    unziped_folder = temp_working_path.joinpath(zip_file.stem)\n",
    "    print(f\"Processing: {unziped_folder}\")\n",
    "\n",
    "    if unziped_folder.exists() and force_clean:\n",
    "        shutil.rmtree(unziped_folder)\n",
    "        print(f\"Removed existing folder: {unziped_folder}\")\n",
    "\n",
    "    # Unpack the archive\n",
    "    shutil.unpack_archive(zip_file, unziped_folder)\n",
    "    print(f\"Unpacked {zip_file} to {unziped_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GeoPackage: /home/sofia/dev/skytruth-30x30/data/data/gadm/raw/temp_preprocess/gadm_410-levels/gadm_410-levels.gpkg\n",
      "Selected layer: ADM_0\n"
     ]
    }
   ],
   "source": [
    "# Select data adm_0, dissolve and save as shp\n",
    "geopackage_file = next(unziped_folder.rglob(\"*.gpkg\"), None)\n",
    "\n",
    "if geopackage_file:\n",
    "    print(f\"Found GeoPackage: {geopackage_file}\")\n",
    "\n",
    "    # Specify the layer to read\n",
    "    layer_name = \"ADM_0\"\n",
    "    gdf = gpd.read_file(geopackage_file, layer=layer_name)\n",
    "    print(f\"Selected layer: {layer_name}\")   \n",
    "    \n",
    "else:\n",
    "    print(\"No GeoPackage file found in the unzipped folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gid_0_and_country(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Updates the GID_0 and COUNTRY values in the GeoDataFrame for dependent territories \n",
    "    with the GID_0 and COUNTRY of their sovereign parent countries.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (gpd.GeoDataFrame): The input GeoDataFrame with 'GID_0' and 'COUNTRY' columns.\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame: The GeoDataFrame with updated 'GID_0' and 'COUNTRY' values for dependent territories.\n",
    "    \"\"\"\n",
    "    # Load the dependency_to_parent mapping\n",
    "    with open(scripts_dir.joinpath('data_commons/data/dependency_to_parent.json'), 'r') as json_file:\n",
    "        dependency_to_parent = json.load(json_file)\n",
    "\n",
    "    # Map GID_0 to the updated values\n",
    "    gdf['GID_0'] = gdf['GID_0'].map(lambda x: dependency_to_parent.get(x, (x, x))[0])\n",
    "    \n",
    "    # Update COUNTRY based on the updated GID_0\n",
    "    gdf['COUNTRY'] = gdf['GID_0'].map(lambda x: {v[0]: v[1] for k, v in dependency_to_parent.items()}.get(x, gdf['COUNTRY'].loc[gdf['GID_0'] == x].values[0]))\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def add_translations(df, translations_csv_path):\n",
    "    translations_df = pd.read_csv(translations_csv_path, keep_default_na=False, na_values=[])\n",
    "    \n",
    "    df = df.merge(translations_df[['code', 'name_es', 'name_fr']], left_on='GID_0', right_on='code', how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign territories to their parent countries\n",
    "gdf_updated = update_gid_0_and_country(gdf)\n",
    "\n",
    "# Dissolve by country\n",
    "gdf_updated = gdf_updated.dissolve(by='COUNTRY').reset_index()\n",
    "\n",
    "# Calculate area\n",
    "gdf_updated = gdf_updated.pipe(calculate_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download country translations\n",
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "\n",
    "translations_csv_url = \"vizzuality_processed_data/gadm/preprocess/locations_translated.csv\"\n",
    "translations_csv_output = input_path.joinpath(translations_csv_url.split(\"/\")[-1])\n",
    "\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=translations_csv_url,\n",
    "    file=translations_csv_output,\n",
    "    operation=\"r\",\n",
    ")\n",
    "\n",
    "translations_path = input_path.joinpath('locations_translated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>geometry</th>\n",
       "      <th>GID_0</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>name_es</th>\n",
       "      <th>name_fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>MULTIPOLYGON (((63.61425 29.46993, 63.60868 29...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>644050.28</td>\n",
       "      <td>Afganistán</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       COUNTRY                                           geometry GID_0  \\\n",
       "0  Afghanistan  MULTIPOLYGON (((63.61425 29.46993, 63.60868 29...   AFG   \n",
       "\n",
       "    area_km2     name_es      name_fr  \n",
       "0  644050.28  Afganistán  Afghanistan  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add translations for country names\n",
    "gdf_translated = add_translations(gdf_updated, translations_path).drop(columns=['code'])\n",
    "gdf_translated.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 204/204 [05:40<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "final_gadm = await simplify_async(gdf_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file\n",
    "final_gadm.to_file(output_file.as_posix(), driver=\"ESRI Shapefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip data\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mpa Atlas intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"mpaatlas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source\n",
    "mpaatlas_url = \"https://guide.mpatlas.org/api/v1/zone/geojson\"\n",
    "mpaatlas_file_name = \"mpatlas_assess_zone.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "input_file = downloadFile(\n",
    "    mpaatlas_url,\n",
    "    input_path,\n",
    "    overwrite=force_clean,\n",
    "    file=mpaatlas_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not force_clean and zipped_output_file.exists():\n",
    "    print(f\"File {zipped_output_file} already exists\")\n",
    "\n",
    "# Transform data\n",
    "gdf = gpd.read_file(input_file)\n",
    "\n",
    "df = (gdf\n",
    "      .pipe(set_wdpa_id)\n",
    "      .pipe(protection_level)\n",
    "      .pipe(status)\n",
    "      .pipe(create_year))\n",
    "\n",
    "df.drop(\n",
    "    columns=list(\n",
    "        set(df.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"wdpa_id\",\n",
    "                \"mpa_zone_id\", \n",
    "                \"name\",\n",
    "                \"designation\",\n",
    "                \"sovereign\",\n",
    "                \"establishment_stage\",\n",
    "                \"protection_mpaguide_level\",\n",
    "                \"protection_level\",\n",
    "                \"year\",\n",
    "                \"geometry\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "df.rename(columns={\"sovereign\": \"location_id\", \"wdpa_pid\": \"wdpa_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3376415/3601108936.py:5: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  ).to_file(filename=output_file.as_posix(), driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
      "/home/sofia/miniforge3/envs/skytruth/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'mpa_zone_id' to 'mpa_zone_i'\n",
      "  ogr_write(\n",
      "/home/sofia/miniforge3/envs/skytruth/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'designation' to 'designatio'\n",
      "  ogr_write(\n",
      "/home/sofia/miniforge3/envs/skytruth/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'location_id' to 'location_i'\n",
      "  ogr_write(\n",
      "/home/sofia/miniforge3/envs/skytruth/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'establishment_stage' to 'establishm'\n",
      "  ogr_write(\n",
      "/home/sofia/miniforge3/envs/skytruth/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'protection_mpaguide_level' to 'protection'\n",
      "  ogr_write(\n",
      "/home/sofia/miniforge3/envs/skytruth/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'protection_level' to 'protecti_1'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "#save data\n",
    "gpd.GeoDataFrame(\n",
    "    df,\n",
    "    crs=gdf.crs,\n",
    ").to_file(filename=output_file.as_posix(), driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
    "\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mpas protected planet intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"mpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpa_url = \"https://www.protectedplanet.net/downloads\"\n",
    "mpa_body = {\n",
    "    \"domain\": \"general\",\n",
    "    \"format\": \"shp\",\n",
    "    \"token\": \"marine\",\n",
    "    \"id\": 21961,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)\n",
    "\n",
    "# # download data\n",
    "# r = requests.post(url=mpa_url, data=mpa_body)\n",
    "# r.raise_for_status()\n",
    "\n",
    "# download_url = r.json().get(\"url\")\n",
    "# input_file_name = f'{r.json().get(\"title\")}.zip'\n",
    "# print(r.json())\n",
    "\n",
    "# input_file =  downloadFile(\n",
    "#     url=download_url,\n",
    "#     output_path=input_path,\n",
    "#     overwrite=force_clean,\n",
    "#     file=input_file_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file twice due how data is provisioned by protected planet\n",
    "shutil.unpack_archive(\n",
    "    input_file,\n",
    "    temp_working_path,\n",
    "    \"zip\",\n",
    ")\n",
    "\n",
    "for file in temp_working_path.glob(\"*.zip\"):\n",
    "    shutil.unpack_archive(file, temp_working_path.joinpath(file.stem), \"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data & Transform it\n",
    "unziped_folders = []\n",
    "for file in temp_working_path.glob(\"*/*.shp\"):\n",
    "    df = (\n",
    "        gpd.read_file(file)\n",
    "        .pipe(filter_by_methodology)\n",
    "        .pipe(transform_points)\n",
    "        .pipe(clean_geometries)\n",
    "    )\n",
    "    unziped_folders.append(df)\n",
    "\n",
    "# merge datasets\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.concat(unziped_folders, ignore_index=True),\n",
    "    crs=unziped_folders[0].crs,\n",
    ")\n",
    "\n",
    "gdf.drop(\n",
    "    columns=list(\n",
    "        set(gdf.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"geometry\",\n",
    "                \"WDPAID\",\n",
    "                \"WDPA_PID\",\n",
    "                \"PA_DEF\",\n",
    "                \"NAME\",\n",
    "                \"PARENT_ISO\",\n",
    "                \"DESIG_ENG\",\n",
    "                \"IUCN_CAT\",\n",
    "                \"STATUS\",\n",
    "                \"STATUS_YR\",\n",
    "                \"GIS_M_AREA\",\n",
    "                \"AREA_KM2\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "gdf[\"WDPAID\"] = pd.to_numeric(gdf[\"WDPAID\"], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data & zip it\n",
    "gdf.to_file(filename=output_file, driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
    "\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pas protected planet intermediate terrestrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"mpa-terrestrial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpa_url = \"https://www.protectedplanet.net/downloads\"\n",
    "mpa_body = {\n",
    "    \"domain\": \"general\",\n",
    "    \"format\": \"shp\",\n",
    "    \"token\": \"wdpa\",\n",
    "    \"id\": 76011,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "# input_path = working_folder.pipe_raw_path\n",
    "input_file = working_folder.pipe_raw_path.joinpath(\"WDPA_Sep2024_Public_shp.zip\")\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"gpkg\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'wdpa-shp', 'title': 'WDPA_Oct2024_Public_shp', 'url': 'https://d1gam3xoknrgr2.cloudfront.net/current/WDPA_Oct2024_Public_shp.zip', 'hasFailed': False, 'token': 'wdpa'}\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "r = requests.post(url=mpa_url, data=mpa_body)\n",
    "r.raise_for_status()\n",
    "\n",
    "download_url = r.json().get(\"url\")\n",
    "input_file_name = f'{r.json().get(\"title\")}.zip'\n",
    "print(r.json())\n",
    "\n",
    "input_file = downloadFile(\n",
    "    url=download_url,\n",
    "    output_path=input_path,\n",
    "    overwrite=force_clean,\n",
    "    file=input_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file twice due how data is provisioned by protected planet\n",
    "shutil.unpack_archive(\n",
    "    input_file,\n",
    "    temp_working_path,\n",
    "    \"zip\",\n",
    ")\n",
    "\n",
    "for file in temp_working_path.glob(\"*.zip\"):\n",
    "    shutil.unpack_archive(file, temp_working_path.joinpath(file.stem), \"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data & Transform it\n",
    "unziped_folders = []\n",
    "for file in temp_working_path.glob(\"*/*.shp\"):\n",
    "    df = (\n",
    "        gpd.read_file(file)\n",
    "        .pipe(filter_by_methodology)\n",
    "        .pipe(filter_by_terrestrial)\n",
    "        .pipe(transform_points)\n",
    "        .pipe(clean_geometries)\n",
    "    )\n",
    "    unziped_folders.append(df)\n",
    "\n",
    "# merge datasets\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.concat(unziped_folders, ignore_index=True),\n",
    "    crs=unziped_folders[0].crs,\n",
    ")\n",
    "\n",
    "gdf.drop(\n",
    "    columns=list(\n",
    "        set(gdf.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"geometry\",\n",
    "                \"WDPAID\",\n",
    "                \"WDPA_PID\",\n",
    "                \"PA_DEF\",\n",
    "                \"NAME\",\n",
    "                \"PARENT_ISO\",\n",
    "                \"DESIG_ENG\",\n",
    "                \"IUCN_CAT\",\n",
    "                \"STATUS\",\n",
    "                \"STATUS_YR\",\n",
    "                \"GIS_AREA\",\n",
    "                \"MARINE\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "gdf[\"WDPAID\"] = pd.to_numeric(gdf[\"WDPAID\"], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 292261/292261 [03:34<00:00, 1362.25it/s]\n"
     ]
    }
   ],
   "source": [
    "final_wdpa_terrestrial = await simplify_async(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data & zip it\n",
    "final_wdpa_terrestrial.to_file(\n",
    "    filename=output_file,\n",
    "    driver=\"GPKG\",\n",
    "    layer=\"name\",\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean unzipped files\n",
    "# rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "# rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protected planet intermediate all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"pa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpa_url = \"https://www.protectedplanet.net/downloads\"\n",
    "mpa_body = {\n",
    "    \"domain\": \"general\",\n",
    "    \"format\": \"shp\",\n",
    "    \"token\": \"wdpa\",\n",
    "    \"id\": 76011,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"gpkg\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'wdpa-shp', 'title': 'WDPA_Sep2024_Public_shp', 'url': 'https://d1gam3xoknrgr2.cloudfront.net/current/WDPA_Sep2024_Public_shp.zip', 'hasFailed': False, 'token': 'wdpa'}\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "r = requests.post(url=mpa_url, data=mpa_body)\n",
    "r.raise_for_status()\n",
    "\n",
    "download_url = r.json().get(\"url\")\n",
    "input_file_name = f'{r.json().get(\"title\")}.zip'\n",
    "print(r.json())\n",
    "\n",
    "# input_file = downloadFile(\n",
    "#     url=download_url,\n",
    "#     output_path=input_path,\n",
    "#     overwrite=force_clean,\n",
    "#     file=input_file_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file twice due how data is provisioned by protected planet\n",
    "shutil.unpack_archive(\n",
    "    input_file,\n",
    "    temp_working_path,\n",
    "    \"zip\",\n",
    ")\n",
    "\n",
    "for file in temp_working_path.glob(\"*.zip\"):\n",
    "    shutil.unpack_archive(file, temp_working_path.joinpath(file.stem), \"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data & Transform it\n",
    "unziped_folders = []\n",
    "for file in temp_working_path.glob(\"*/*.shp\"):\n",
    "    df = (\n",
    "        gpd.read_file(file)\n",
    "        .pipe(filter_by_methodology)\n",
    "        .pipe(transform_points)\n",
    "        .pipe(clean_geometries)\n",
    "    )\n",
    "    unziped_folders.append(df)\n",
    "\n",
    "# merge datasets\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.concat(unziped_folders, ignore_index=True),\n",
    "    crs=unziped_folders[0].crs,\n",
    ")\n",
    "\n",
    "gdf.drop(\n",
    "    columns=list(\n",
    "        set(gdf.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"geometry\",\n",
    "                \"WDPAID\",\n",
    "                \"WDPA_PID\",\n",
    "                \"PA_DEF\",\n",
    "                \"NAME\",\n",
    "                \"PARENT_ISO\",\n",
    "                \"DESIG_ENG\",\n",
    "                \"IUCN_CAT\",\n",
    "                \"STATUS\",\n",
    "                \"STATUS_YR\",\n",
    "                \"GIS_AREA\",\n",
    "                \"GIS_M_AREA\",\n",
    "                \"MARINE\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "gdf[\"WDPAID\"] = pd.to_numeric(gdf[\"WDPAID\"], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 298912/298912 [03:53<00:00, 1277.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                                                                                            | 1817/298912 [00:11<03:42, 1338.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                                           | 3731/298912 [00:12<03:25, 1433.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Polygon' object has no attribute 'geoms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▏                                                                                                                                                          | 4223/298912 [00:12<01:23, 3536.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██████                                                                                                                                                      | 11698/298912 [00:15<04:00, 1191.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████████████████████                                                                                                                            | 61318/298912 [00:27<03:03, 1298.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████████████████████▉                                                                                                                         | 66972/298912 [00:29<02:28, 1566.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████████████████████████████▌                                                                                                         | 96777/298912 [00:35<01:04, 3139.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████████████████████▏                                                                                                     | 102462/298912 [00:37<00:46, 4270.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████████████████████████████████████████▍                                                                                                   | 106818/298912 [00:38<01:33, 2059.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████████████████████████████████████████▉                                                                                            | 121477/298912 [00:41<01:20, 2212.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████████████████████████████████                                                                                        | 129353/298912 [00:44<01:10, 2404.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Polygon' object has no attribute 'geoms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████████████████████████████████████▊                                                                                    | 136616/298912 [00:46<01:06, 2457.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Polygon' object has no attribute 'geoms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████▊                                                                              | 148130/298912 [00:50<01:02, 2399.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████████████████████████████████████████████████████████████████▍                                                                            | 151376/298912 [00:51<01:09, 2121.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████████████████████████████████████████████                                                                        | 160280/298912 [00:53<01:55, 1197.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Polygon' object has no attribute 'geoms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                     | 164997/298912 [00:54<01:16, 1760.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Polygon' object has no attribute 'geoms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████████████████████▍                                                                    | 166577/298912 [00:55<01:03, 2072.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████████████████████████████████████████████████████████████████████▌                                                                 | 172769/298912 [00:56<01:01, 2037.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████████████████████████████████████████████████████████████████████████▎                                                                | 174238/298912 [00:57<00:30, 4024.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 201035/298912 [01:03<00:17, 5566.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                | 205073/298912 [01:04<00:17, 5454.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 210501/298912 [01:05<00:27, 3184.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 225210/298912 [01:08<00:17, 4259.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                     | 225947/298912 [01:09<00:48, 1498.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                    | 229699/298912 [01:10<00:23, 2896.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 252333/298912 [01:15<00:15, 3001.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Polygon' object has no attribute 'geoms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 277458/298912 [01:21<00:07, 2831.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 281117/298912 [01:22<00:12, 1449.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n",
      "<class 'shapely.geometry.base.GeometrySequence'>\n",
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 284183/298912 [01:23<00:04, 3294.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 296106/298912 [01:32<00:00, 3532.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.base.GeometrySequence'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 298912/298912 [03:53<00:00,  2.59it/s]"
     ]
    }
   ],
   "source": [
    "final_wdpa = await simplify_async(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data & zip it\n",
    "final_wdpa.to_file(\n",
    "    filename=output_file,\n",
    "    driver=\"GPKG\",\n",
    "    layer=\"name\",\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habitats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"habitats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "habitats_download_url = \"https://habitats.oceanplus.org/downloads/global_statistics.zip\"\n",
    "Mangroves_download_url = \"https://mangrove-atlas-api.herokuapp.com/admin/widget_protected_areas.csv\"\n",
    "mangroves_request_headers = {\n",
    "    \"Cookie\": \"_mangrove_atlas_api_session=fJuobvI2fH42WfGfMtRTp%2BksIDdPEpY6DG8uCuITsENtrRGG4AA3nYEeAI7dytzpK%2F0dGIHq84O54MRr6eiPgiwCYXp2XP4IzXM40dFt%2FI6hoB0WXC%2Fwrd81XreNnMZiSEE6IVT5R0fqMcmsZdPn53u0A1d4CGU3FfliOZuWkckBuA%2F7C4upBGuSS8817LqOh1slG%2BsEOGp3nk7WX4fMoPbsHWtARfFwdfoAHz448LO7uWuZdyiu7YOrS0ZxOZEb9JZ8hcUJph4pBFofZLpOvtQQutgZY21T5bhQ7Kwfl56e6Qr0SZ%2B8sIzMfky3h%2FjOA6DNTLoy%2BZLiZBAgFHlTYm2JwlwqWgAZU8D7cE7Zn%2Fxgf3LFF9pZ9Fe3QG4c8LIwH%2FxqjEd8GsZAhBMgBWbxubigQ9gZssZt6CIO--7qiVsTAT8JAKj1jU--U7TI%2Fz9c151bfD8iZdkBDw%3D%3D\"\n",
    "}\n",
    "seamounts_download_url = \"https://datadownload-production.s3.amazonaws.com/ZSL002_ModelledSeamounts2011_v1.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seamounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seamounts_path = input_path.joinpath(\"seamounts\")\n",
    "input_seamounts_path.mkdir(parents=True, exist_ok=True)\n",
    "# download data\n",
    "input_file_name = \"seamounts.zip\"\n",
    "input_file = downloadFile(\n",
    "    url=seamounts_download_url,\n",
    "    output_path=input_seamounts_path,\n",
    "    overwrite=force_clean,\n",
    "    file=input_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip data\n",
    "shutil.unpack_archive(\n",
    "    input_file,\n",
    "    temp_working_path,\n",
    "    \"zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_working_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first =gpd.read_file(next(temp_working_path.rglob(\"*SeamountsBaseArea.shp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not force_clean and zipped_output_file.exists():\n",
    "    print(f\"File {zipped_output_file} already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
